# Professional Nano-LLM Engine - Project Configuration

project:
  name: "Professional Nano-LLM Engine"
  version: "0.1.0"
  description: "Enterprise-grade LLM inference engine based on nano-vLLM"
  license: "MIT"
  author: "Professional AI Development Team"

# Development Configuration
development:
  python_version: "3.9+"
  dependencies:
    core:
      - torch>=2.0.0
      - transformers>=4.30.0
      - fastapi>=0.100.0
      - uvicorn>=0.22.0
      - pydantic>=2.0.0
    monitoring:
      - prometheus-client>=0.17.0
      - grafana-api>=1.0.3
    testing:
      - pytest>=7.0.0
      - pytest-asyncio>=0.21.0
      - pytest-cov>=4.0.0
    security:
      - cryptography>=40.0.0
      - python-jose>=3.3.0
      - passlib>=1.7.4

# Environment Configurations
environments:
  development:
    debug: true
    log_level: "DEBUG"
    database_url: "sqlite:///./dev.db"
    redis_url: "redis://localhost:6379"
    
  staging:
    debug: false
    log_level: "INFO"
    database_url: "postgresql://user:pass@staging-db:5432/professional_nano_llm"
    redis_url: "redis://staging-redis:6379"
    
  production:
    debug: false
    log_level: "WARNING"
    database_url: "postgresql://user:pass@prod-db:5432/professional_nano_llm"
    redis_url: "redis://prod-redis:6379"

# Performance Targets
performance:
  targets:
    latency_p95: "100ms"      # 95th percentile latency
    throughput: "2000 tokens/s" # Target throughput
    memory_usage: "80%"       # Max GPU memory usage
    availability: "99.9%"     # Target uptime
    
  benchmarks:
    baseline_model: "Qwen/Qwen3-0.6B"
    test_sequences: 256
    input_length_range: [100, 1024]
    output_length_range: [100, 1024]

# Security Configuration
security:
  authentication:
    jwt_secret_key: "${JWT_SECRET_KEY}"
    jwt_algorithm: "HS256"
    access_token_expire_minutes: 30
    
  rate_limiting:
    free_tier:
      requests_per_minute: 60
      tokens_per_day: 10000
    pro_tier:
      requests_per_minute: 600
      tokens_per_day: 100000
    enterprise:
      requests_per_minute: -1  # Unlimited
      tokens_per_day: -1       # Unlimited

# Monitoring Configuration
monitoring:
  metrics:
    enabled: true
    export_port: 8000
    
  logging:
    format: "json"
    level: "INFO"
    file_rotation: "1 day"
    max_files: 30
    
  alerts:
    latency_threshold: "200ms"
    error_rate_threshold: "1%"
    memory_threshold: "90%"

# Deployment Configuration
deployment:
  docker:
    base_image: "nvidia/cuda:12.1.1-devel-ubuntu22.04"
    python_version: "3.9"
    
  kubernetes:
    namespace: "professional-nano-llm"
    replicas: 3
    resources:
      requests:
        memory: "8Gi"
        cpu: "2"
        nvidia.com/gpu: "1"
      limits:
        memory: "16Gi"
        cpu: "4"
        nvidia.com/gpu: "1"

# Feature Flags
features:
  multi_model_support: true
  a_b_testing: true
  context_management: true
  streaming_api: true
  batch_inference: true
  custom_optimizations: true
